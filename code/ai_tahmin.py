import cv2
import mediapipe as mp
import numpy as np
import tensorflow as tf
import time
import os
import json
from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume
from comtypes import CLSCTX_ALL
import pyautogui
import screen_brightness_control as sbc
from datetime import datetime

# ---------- AYARLAR ----------
MODEL_PATH = "gesture_model.keras"
DATASET_DIR = "dataset"
PREPARATION_DURATION = 2  # saniye
COOLDOWN = 3  # saniye (2'den 3'e Ã§Ä±karÄ±ldÄ±)
MOD_CONFIG_PATH = "mod_config.json"
# ------------------------------

# ğŸ’¾ Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ klasÃ¶rÃ¼ (varsayÄ±lan 'Resimler/Ekran GÃ¶rÃ¼leri')
screenshot_dir = r"C:\Users\mlhyl\OneDrive\Resimler\Ekran GÃ¶rÃ¼ntÃ¼leri"
os.makedirs(screenshot_dir, exist_ok=True)  # klasÃ¶r yoksa oluÅŸtur


# --- Mod SeÃ§imi Okuma ---
def get_active_mode():
    try:
        with open(MOD_CONFIG_PATH, "r") as f:
            return json.load(f).get("mode", "computer")
    except FileNotFoundError:
        return "computer"


# 1) Mediapipe el tanÄ±ma
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=1,
    min_detection_confidence=0.7,
    min_tracking_confidence=0.7
)

# 2) Modeli yÃ¼kle ve giriÅŸ boyutunu al
model = tf.keras.models.load_model(MODEL_PATH)
INPUT_DIM = model.input_shape[1]  # eÄŸitimde belirlenen 42

# 3) SÄ±nÄ±f adlarÄ±nÄ± dataset klasÃ¶rÃ¼nden oku
gesture_names = sorted([
    os.path.splitext(f)[0]
    for f in os.listdir(DATASET_DIR)
    if f.lower().endswith(".csv")
])
print("ğŸ”¢ Bulunan gesture sÄ±nÄ±flarÄ±:", gesture_names)


# 4) Ses kontrol fonksiyonlarÄ± (Windows)
def set_volume(level):
    dev = AudioUtilities.GetSpeakers()
    iface = dev.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)
    vol = iface.QueryInterface(IAudioEndpointVolume)
    vol.SetMasterVolumeLevelScalar(level, None)


def unmute():
    print("ğŸ”Š Ses AÃ§Ä±ldÄ±!")
    set_volume(1.0)


def mute():
    print("ğŸ”‡ Ses KapatÄ±ldÄ±!")
    set_volume(0.0)


# --- 5) Modlara GÃ¶re Gesture-Komut SÃ¶zlÃ¼kleri ---

# Bilgisayar (PC) modu - feedback mesajlarÄ± eklendi
gesture_actions_pc = {
    "0": (unmute, "Sound turned up!"),
    "1": (mute, "Sound turned off!"),
    "2": (
        lambda: (sbc.set_brightness(100), print("Brightness increased!")),
        "Brightness increased!"
    ),
    "3": (
        lambda: (sbc.set_brightness(0), print("Brightness decreased!")),
        "Brightness decreased!"
    ),
    "4": (
        lambda: (
            pyautogui.screenshot(
                os.path.join(screenshot_dir,
                             f"screenshot_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
            ),
            print("Screenshot saved!")
        ),
        "Screenshot saved!"
    ),
    "5": (
        lambda: (
            print("ğŸ›‘ Ã‡Ä±kÄ±lÄ±yor..."),
            cap.release(),
            cv2.destroyAllWindows(),
            exit()
        ),
        "Exiting..."
    )
}


# Medya modu - feedback mesajlarÄ± eklendi
gesture_actions_media = {
    "0": (lambda: print("â¯ï¸ Play/Pause"), "â¯ï¸ Play/Pause"),
    "1": (lambda: print("ğŸ”‡ Mute/Unmute"), "ğŸ”‡ Mute/Unmute"),
    "2": (lambda: print("â­ï¸ Next Track"), "â­ï¸ Next Track"),
    "3": (lambda: print("â®ï¸ Previous Track"), "â®ï¸ Previous Track"),
    "4": (lambda: print("ğŸ”Š Volume Up / Down Toggle"), "ğŸ”Š Volume Up/Down")
}

# Slayt (Sunum) modu - feedback mesajlarÄ± eklendi
gesture_actions_slide = {
    "0": (lambda: print("â–¶ï¸ Start Presentation"), "â–¶ï¸ Presentation Started"),
    "1": (lambda: print("âŒ End Presentation"), "âŒ Presentation Ended"),
    "2": (lambda: print("â¡ Next Slide"), "â¡ Next Slide"),
    "3": (lambda: print("â¬… Previous Slide"), "â¬… Previous Slide"),
    "4": (lambda: print("ğŸ”’ Lock Slide / Highlight"), "ğŸ”’ Slide Locked")
}


# 6) Landmark â†’ 42-boyut Ã¶zellik vektÃ¶rÃ¼ fonksiyonu
def get_feature_vector(landmarks):
    x = np.array([lm.x for lm in landmarks])
    y = np.array([lm.y for lm in landmarks])
    nx = (x - x.min()) / (x.max() - x.min() + 1e-6)
    ny = (y - y.min()) / (y.max() - y.min() + 1e-6)
    inter = np.empty((42,), dtype=np.float32)
    inter[0::2] = nx
    inter[1::2] = ny
    return inter.reshape(1, INPUT_DIM)


# 7) Kamera ve zamanlayÄ±cÄ±lar
cap = cv2.VideoCapture(0)
gesture_start = None
last_trigger = 0
waiting = False
feedback_message = ""  # Feedback mesajÄ± iÃ§in yeni deÄŸiÅŸken

# ---------- ANA DÃ–NGÃœ ----------
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    frame = cv2.flip(frame, 1)
    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    res = hands.process(rgb)
    now = time.time()

    # cooldown kontrolÃ¼
    if waiting and now - last_trigger < COOLDOWN:
        txt = f"Please {COOLDOWN - (now - last_trigger):.1f}s wait"
        cv2.putText(frame, txt, (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)

        # SaÄŸ alt kÃ¶ÅŸeye feedback mesajÄ± ekleme
        if feedback_message:
            cv2.putText(frame, feedback_message,
                        (frame.shape[1] - 400, frame.shape[0] - 20),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)

        cv2.imshow("Gesture Recognition", frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
        continue
    waiting = False
    feedback_message = ""  # Feedback mesajÄ±nÄ± sÄ±fÄ±rla

    # el algÄ±lama ve hazÄ±rlÄ±k sÃ¼resi
    if res.multi_hand_landmarks:
        if gesture_start is None:
            gesture_start = now

        elapsed = now - gesture_start
        remaining = max(0, PREPARATION_DURATION - elapsed)

        if elapsed < PREPARATION_DURATION:
            cv2.putText(frame,
                        f"Get ready! {remaining:.1f}s",
                        (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
        else:
            # tahmin zamanÄ±
            for hand in res.multi_hand_landmarks:
                inp = get_feature_vector(hand.landmark)
                preds = model.predict(inp, verbose=0)
                cid = int(np.argmax(preds))
                name = gesture_names[cid] if cid < len(gesture_names) else "Bilinmeyen"

                cv2.putText(frame, name, (20, 40),
                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                print(f"â†’ Tahmin: {name} (id={cid}), olasÄ±lÄ±klar: {preds}")

                # aktif moda gÃ¶re komut Ã§alÄ±ÅŸtÄ±r
                mode = get_active_mode()
                if mode == "computer" and name in gesture_actions_pc:
                    action, feedback_message = gesture_actions_pc[name]
                    action()
                elif mode == "media" and name in gesture_actions_media:
                    action, feedback_message = gesture_actions_media[name]
                    action()
                elif mode == "presentation" and name in gesture_actions_slide:
                    action, feedback_message = gesture_actions_slide[name]
                    action()
                else:
                    # tanÄ±nmayan hareket veya geÃ§ersiz mod
                    feedback_message = ""

                last_trigger = now
                waiting = True

            gesture_start = None
    else:
        gesture_start = None
        cv2.putText(frame, "Hand is not detected!",
                    (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

    # Modu ekrana yazdÄ±rma
    mode = get_active_mode()
    cv2.putText(frame, f"Mod: {mode.capitalize()}", (20, 120),
                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)

    cv2.imshow("Gesture Recognition", frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()